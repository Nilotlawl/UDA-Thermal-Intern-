{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZoqBjoMoGpn0Dv4Ek0RdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilotpalMaitra/UDA-Thermal-Intern-/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/qwedaq/UDA_thermal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWnifuaul-Ty",
        "outputId": "0755f9c4-8463-4dc6-ca46-375ea736064c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UDA_thermal'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 5), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 5.71 MiB | 9.00 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5a04W10mEWF",
        "outputId": "7e912643-2055-4f4d-ddb4-f3bcaa56317a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py\t\t\t\t\t      models\t      README.md  tsne.py\n",
            "mnist_mnistm_model1_epoch_192_best_digit.pth  prec_recall.py  test.py\t UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgjbZlCmKQl",
        "outputId": "21844619-e0dc-49ad-f70e-c577e1a46c5a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting models\n",
            "  Using cached models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd UDA_thermal/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mlEVRksmOXE",
        "outputId": "cb640510-eda5-4438-a331-7c972c1c0fa9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rE3QQXKmSli",
        "outputId": "0ba6e033-33df-41f9-8ff1-192ab14dd454"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py\t\t\t\t\t      models\t      README.md  tsne.py\n",
            "mnist_mnistm_model1_epoch_192_best_digit.pth  prec_recall.py  test.py\t UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mlO-9nWdluom"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from models.functions import ReverseLayerF\n",
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3sMhX70Sl3_P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: For the above class BasciConv , how do i check the output of the code, to check whether the code is running or not perfectly?\n",
        "\n",
        "# Create an instance of the BasicConv class\n",
        "basic_conv = BasicConv(3, 64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "\n",
        "# Pass the input tensor through the BasicConv module\n",
        "output_tensor = basic_conv(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (1, 64, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"BasicConv module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-WpZOhgnil2",
        "outputId": "72eda244-ac21-46bf-d055-27cf770f8b00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 112, 112])\n",
            "BasicConv module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "0shbRl8_XaFN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type=='avg':\n",
        "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( avg_pool )\n",
        "            elif pool_type=='max':\n",
        "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( max_pool )\n",
        "            elif pool_type=='lp':\n",
        "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( lp_pool )\n",
        "            elif pool_type=='lse':\n",
        "                # LSE pool only\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp( lse_pool )\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "JAR_k7VpmbVN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How to check the output of the channel gate?\n",
        "\n",
        "# Create an instance of the ChannelGate class\n",
        "channel_gate = ChannelGate(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelGate module\n",
        "output_tensor = channel_gate(input_tensor)\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelGate module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cksjekign6pB",
        "outputId": "503e0e32-0364-40c6-e473-b3e58ca1dc98"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChannelGate module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out) # broadcasting\n",
        "        return x * scale\n"
      ],
      "metadata": {
        "id": "abWFYSrzmlJS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class ChannelPool\n",
        "\n",
        "# Create an instance of the ChannelPool class\n",
        "channel_pool = ChannelPool()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelPool module\n",
        "output_tensor = channel_pool(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (1, 2, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelPool module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFdbFVHcoHy5",
        "outputId": "dca675e1-6717-480f-f181-507d7d37c23a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 112, 112])\n",
            "ChannelPool module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse'], no_spatial=True):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
        "        self.no_spatial=no_spatial\n",
        "        #if not no_spatial:\n",
        "            #self.SpatialGate = SpatialGate()\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        #if not self.no_spatial:\n",
        "            #x_out = self.SpatialGate(x_out)\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "LBNulbajmoeX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class CBAM module\n",
        "\n",
        "# Create an instance of the CBAM class\n",
        "cbam = CBAM(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the CBAM module\n",
        "output_tensor = cbam(input_tensor)\n",
        "\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CBAM module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UQaaFiVngMa",
        "outputId": "12a48cd1-5d82-416d-b53a-295b3997a615"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBAM module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        pre_a =  models.alexnet(pretrained=True)\n",
        "        self.features = pre_a.features[0:8]\n",
        "        for i,p in enumerate(self.features.parameters()):\n",
        "            #if(i<6): #[changed to [2,4,6] depending on how many conv layers to freeze], 132 for mobilenet\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(128) #128 for digit 256 for alpha\n",
        "        self.bn2 = nn.BatchNorm2d(64) # 64 for digit 128 for alpha\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.cbam1 = CBAM(128) #128 for digit 256 for alpha\n",
        "        self.cbam2 = CBAM(64) # 64 for digit 128 for alpha\n",
        "\n",
        "        self.bottleneck2 = nn.Sequential()\n",
        "        self.bottleneck2.add_module('b2_conv1',nn.Conv2d(128, 32, kernel_size=1,stride=1,padding = 'same')) # change with 128 and 32 for digit, 256 and 64 for alpha\n",
        "        #self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn1',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv2',nn.Conv2d(32, 32, kernel_size=3,stride=1,padding = 'same',dilation=2))\n",
        "        #self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn2',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv3',nn.Conv2d(32, 128, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn3',nn.BatchNorm2d(128))\n",
        "        self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "\n",
        "        self.bottleneck4 = nn.Sequential()\n",
        "        self.bottleneck4.add_module('b4_conv1',nn.Conv2d(64, 16, kernel_size=1,stride=1,padding = 'same')) # change with 64 and 16 for digit, 128 and 32 for alpha\n",
        "        #self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn1',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv2',nn.Conv2d(16, 16, kernel_size=3,stride=1,padding = 'same', dilation=2))\n",
        "        #self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn2',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv3',nn.Conv2d(16, 64, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn3',nn.BatchNorm2d(64))\n",
        "        self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        #'''\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(256, 100))  #256 for digit 128*4*4 for aplha\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100)) # 500 for alpha\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        #self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10)) #10 for digit and 4 for alpha\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax())\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(128*4*4, 100))#256 for digit 128*4*4 for aplha\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "        #self.conv3 =\n",
        "        self.conv4 = nn.Conv2d(256, 128, kernel_size=3,dilation=2) # change back to square filter and dilation 2 for digit\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3,dilation=2)\n",
        "        self.max3 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "    def forward(self, input_data, alpha):\n",
        "        #input_data = input_data.expand(input_data.data.shape[0], 3, 32, 32)\n",
        "        feature = self.features(input_data)\n",
        "        #feature = self.feature(feature)\n",
        "\n",
        "        feature = self.relu(self.bn1(self.conv4(feature)))\n",
        "        feature = self.bottleneck2(feature)\n",
        "        #feature = feature_o1 + feature_n1\n",
        "        feature = self.cbam1(feature)\n",
        "        feature = self.relu(self.bn2(self.conv5(feature)))\n",
        "        feature = self.bottleneck4(feature)\n",
        "        #feature = feature_o2 + feature_n2\n",
        "        feature = self.cbam2(feature)\n",
        "        feature = self.max3(feature)\n",
        "\n",
        "\n",
        "        feature = feature.view(-1, 256*4*4) #256 for digit\n",
        "        reverse_feature = ReverseLayerF.apply(feature, 1.0)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "        #print(class_output.shape)\n",
        "        return class_output, domain_output"
      ],
      "metadata": {
        "id": "fCkAxiEgV1CH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Code to check the output of class CNN model\n",
        "\n",
        "# Create an instance of the CNNModel class\n",
        "cnn_model = CNNModel()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Create a random alpha value\n",
        "alpha = torch.randn(1)\n",
        "\n",
        "# Pass the input tensor and alpha value through the CNNModel module\n",
        "class_output, domain_output = cnn_model(input_tensor, alpha)\n",
        "\n",
        "# Print the shapes of the class_output and domain_output tensors\n",
        "print(class_output.shape)\n",
        "print(domain_output.shape)\n",
        "\n",
        "# Check if the class_output tensor has the expected shape\n",
        "expected_class_shape = (1, 10)  # 10 for digit and 4 for alpha\n",
        "assert class_output.shape == expected_class_shape, \"Class output tensor has unexpected shape\"\n",
        "\n",
        "# Check if the domain_output tensor has the expected shape\n",
        "expected_domain_shape = (1, 2)\n",
        "assert domain_output.shape == expected_domain_shape, \"Domain output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CNNModel module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qD2OwNmTXoSg",
        "outputId": "5d08af0a-077f-425d-c03f-0e06f871531e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 122MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [128, 256, 3, 3], expected input[1, 384, 13, 13] to have 256 channels, but got 384 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4d9a93fd17ca>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Pass the input tensor and alpha value through the CNNModel module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclass_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the shapes of the class_output and domain_output tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-13e443324225>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, alpha)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#feature = self.feature(feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottleneck2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m#feature = feature_o1 + feature_n1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 256, 3, 3], expected input[1, 384, 13, 13] to have 256 channels, but got 384 channels instead"
          ]
        }
      ]
    }
  ]
}