{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0i0GfA8g79I3jFdUZG9YX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilotpalMaitra/UDA-Thermal-Intern-/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/qwedaq/UDA_thermal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWnifuaul-Ty",
        "outputId": "8cbbc6c8-9eb5-4758-cece-66a99e59a4b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UDA_thermal'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 5), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 5.71 MiB | 2.56 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5a04W10mEWF",
        "outputId": "7f9c4379-85d4-49d8-efd6-6a5f3da587f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgjbZlCmKQl",
        "outputId": "52d21c3a-b715-4d2a-c790-909f0af45a02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting models\n",
            "  Downloading models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd UDA_thermal/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mlEVRksmOXE",
        "outputId": "762f0656-5c21-4ac1-fd08-0d22a1623b2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rE3QQXKmSli",
        "outputId": "7098ebf4-931b-401d-df04-0224302e745b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py\t\t\t\t\t      models\t      README.md  tsne.py\n",
            "mnist_mnistm_model1_epoch_192_best_digit.pth  prec_recall.py  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mlO-9nWdluom"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from models.functions import ReverseLayerF\n",
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3sMhX70Sl3_P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: For the above class BasciConv , how do i check the output of the code, to check whether the code is running or not perfectly?\n",
        "\n",
        "# Create an instance of the BasicConv class\n",
        "basic_conv = BasicConv(3, 64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "\n",
        "# Pass the input tensor through the BasicConv module\n",
        "output_tensor = basic_conv(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (1, 64, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"BasicConv module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-WpZOhgnil2",
        "outputId": "168755d0-d236-4b30-8ced-8fc8a34951b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 112, 112])\n",
            "BasicConv module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "0shbRl8_XaFN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type=='avg':\n",
        "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( avg_pool )\n",
        "            elif pool_type=='max':\n",
        "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( max_pool )\n",
        "            elif pool_type=='lp':\n",
        "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( lp_pool )\n",
        "            elif pool_type=='lse':\n",
        "                # LSE pool only\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp( lse_pool )\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "JAR_k7VpmbVN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How to check the output of the channel gate?\n",
        "\n",
        "# Create an instance of the ChannelGate class\n",
        "channel_gate = ChannelGate(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelGate module\n",
        "output_tensor = channel_gate(input_tensor)\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelGate module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cksjekign6pB",
        "outputId": "98e1187e-6b9b-4c30-dfe8-7b8d77b3ae99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChannelGate module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out) # broadcasting\n",
        "        return x * scale\n"
      ],
      "metadata": {
        "id": "abWFYSrzmlJS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class ChannelPool\n",
        "\n",
        "# Create an instance of the ChannelPool class\n",
        "channel_pool = ChannelPool()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelPool module\n",
        "output_tensor = channel_pool(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (1, 2, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelPool module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFdbFVHcoHy5",
        "outputId": "846459df-554b-4a04-88b6-affa8b90bca6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 112, 112])\n",
            "ChannelPool module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse'], no_spatial=True):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
        "        self.no_spatial=no_spatial\n",
        "        #if not no_spatial:\n",
        "            #self.SpatialGate = SpatialGate()\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        #if not self.no_spatial:\n",
        "            #x_out = self.SpatialGate(x_out)\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "LBNulbajmoeX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class CBAM module\n",
        "\n",
        "# Create an instance of the CBAM class\n",
        "cbam = CBAM(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the CBAM module\n",
        "output_tensor = cbam(input_tensor)\n",
        "\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CBAM module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UQaaFiVngMa",
        "outputId": "5c7b5a44-41f7-4043-933f-8c066f394c95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBAM module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        pre_a =  models.alexnet(pretrained=True)\n",
        "        self.features = pre_a.features[0:8]\n",
        "        for i,p in enumerate(self.features.parameters()):\n",
        "            #if(i<6): #[changed to [2,4,6] depending on how many conv layers to freeze], 132 for mobilenet\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(128) #128 for digit 256 for alpha\n",
        "        self.bn2 = nn.BatchNorm2d(64) # 64 for digit 128 for alpha\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.cbam1 = CBAM(128) #128 for digit 256 for alpha\n",
        "        self.cbam2 = CBAM(64) # 64 for digit 128 for alpha\n",
        "\n",
        "        self.bottleneck2 = nn.Sequential()\n",
        "        self.bottleneck2.add_module('b2_conv1',nn.Conv2d(128, 32, kernel_size=1,stride=1,padding = 'same')) # change with 128 and 32 for digit, 256 and 64 for alpha\n",
        "        #self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn1',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv2',nn.Conv2d(32, 32, kernel_size=3,stride=1,padding = 'same',dilation=2))\n",
        "        #self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn2',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv3',nn.Conv2d(32, 128, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn3',nn.BatchNorm2d(128))\n",
        "        self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "\n",
        "        self.bottleneck4 = nn.Sequential()\n",
        "        self.bottleneck4.add_module('b4_conv1',nn.Conv2d(64, 16, kernel_size=1,stride=1,padding = 'same')) # change with 64 and 16 for digit, 128 and 32 for alpha\n",
        "        #self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn1',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv2',nn.Conv2d(16, 16, kernel_size=3,stride=1,padding = 'same', dilation=2))\n",
        "        #self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn2',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv3',nn.Conv2d(16, 64, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn3',nn.BatchNorm2d(64))\n",
        "        self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        #'''\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(256, 100))  #256 for digit 128*4*4 for aplha\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100)) # 500 for alpha\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        #self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10)) #10 for digit and 4 for alpha\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax())\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(256, 100))#256 for digit 128*4*4 for aplha\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "        #self.conv3 =\n",
        "        self.conv4 = nn.Conv2d(384, 128, kernel_size=3,dilation=2) # change back to square filter and dilation 2 for digit\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3,dilation=2)\n",
        "        self.max3 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "    def forward(self, input_data, alpha):\n",
        "        #input_data = input_data.expand(input_data.data.shape[0], 3, 32, 32)\n",
        "        feature = self.features(input_data)\n",
        "        #feature = self.feature(feature)\n",
        "\n",
        "        feature = self.relu(self.bn1(self.conv4(feature)))\n",
        "        feature = self.bottleneck2(feature)\n",
        "        #feature = feature_o1 + feature_n1\n",
        "        feature = self.cbam1(feature)\n",
        "        feature = self.relu(self.bn2(self.conv5(feature)))\n",
        "        feature = self.bottleneck4(feature)\n",
        "        #feature = feature_o2 + feature_n2\n",
        "        feature = self.cbam2(feature)\n",
        "        feature = self.max3(feature)\n",
        "\n",
        "\n",
        "        feature = feature.view(-1, 256) #256 for digit\n",
        "        reverse_feature = ReverseLayerF.apply(feature, 1.0)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "        #print(class_output.shape)\n",
        "        return class_output, domain_output"
      ],
      "metadata": {
        "id": "fCkAxiEgV1CH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Code to check the output of class CNN model\n",
        "\n",
        "# Create an instance of the CNNModel class\n",
        "cnn_model = CNNModel()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Create a random alpha value\n",
        "alpha = torch.randn(1)\n",
        "\n",
        "# Pass the input tensor and alpha value through the CNNModel module\n",
        "class_output, domain_output = cnn_model(input_tensor, alpha)\n",
        "\n",
        "# Print the shapes of the class_output and domain_output tensors\n",
        "print(class_output.shape)\n",
        "print(domain_output.shape)\n",
        "\n",
        "# Check if the class_output tensor has the expected shape\n",
        "expected_class_shape = (1, 10)  # 10 for digit and 4 for alpha\n",
        "assert class_output.shape == expected_class_shape, \"Class output tensor has unexpected shape\"\n",
        "\n",
        "# Check if the domain_output tensor has the expected shape\n",
        "expected_domain_shape = (1, 2)\n",
        "assert domain_output.shape == expected_domain_shape, \"Domain output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CNNModel module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qD2OwNmTXoSg",
        "outputId": "80c3812d-df33-41c1-e0b5-a8ee060fdba2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 100])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4d9a93fd17ca>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Pass the input tensor and alpha value through the CNNModel module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclass_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the shapes of the class_output and domain_output tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-be4b3c2a3c67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, alpha)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#256 for digit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mreverse_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReverseLayerF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mclass_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mdomain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverse_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#print(class_output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2507\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected more than 1 value per channel when training, got input size {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 100])"
          ]
        }
      ]
    }
  ]
}