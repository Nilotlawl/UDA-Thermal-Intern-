{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOenuAl4+s7TDkhH4XQ3okl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilotpalMaitra/UDA-Thermal-Intern-/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgya_Psyfpyf",
        "outputId": "60e2f143-c349-4d55-a8be-cf95eb03ed89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n"
      ],
      "metadata": {
        "id": "kW0y9LDrfvgj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n9SQTpxfyrl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/qwedaq/UDA_thermal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWnifuaul-Ty",
        "outputId": "f33d65a6-8a73-4365-9da1-54cb79aab41b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UDA_thermal'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 5), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 5.71 MiB | 30.63 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5a04W10mEWF",
        "outputId": "1fcc42b4-b45a-463c-bd35-c822d610e2a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgjbZlCmKQl",
        "outputId": "848c4a28-d0b1-4acf-e796-88e9b9f49c79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting models\n",
            "  Downloading models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd UDA_thermal/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mlEVRksmOXE",
        "outputId": "6fcbc4f7-3dbd-423d-d7ab-193368f86141"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UDA_thermal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rE3QQXKmSli",
        "outputId": "b519b0e2-7592-4b22-9834-49c909a3438f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py\t\t\t\t\t      models\t      README.md  tsne.py\n",
            "mnist_mnistm_model1_epoch_192_best_digit.pth  prec_recall.py  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mlO-9nWdluom"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from models.functions import ReverseLayerF\n",
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3sMhX70Sl3_P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: For the above class BasciConv , how do i check the output of the code, to check whether the code is running or not perfectly?\n",
        "\n",
        "# Create an instance of the BasicConv class\n",
        "basic_conv = BasicConv(3, 64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(5, 3, 224, 224)\n",
        "\n",
        "\n",
        "# Pass the input tensor through the BasicConv module\n",
        "output_tensor = basic_conv(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (5, 64, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"BasicConv module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-WpZOhgnil2",
        "outputId": "b8a2a5a1-53c0-49dd-a1d4-330305171abc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64, 112, 112])\n",
            "BasicConv module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "0shbRl8_XaFN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type=='avg':\n",
        "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( avg_pool )\n",
        "            elif pool_type=='max':\n",
        "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( max_pool )\n",
        "            elif pool_type=='lp':\n",
        "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( lp_pool )\n",
        "            elif pool_type=='lse':\n",
        "                # LSE pool only\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp( lse_pool )\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "JAR_k7VpmbVN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How to check the output of the channel gate?\n",
        "\n",
        "# Create an instance of the ChannelGate class\n",
        "channel_gate = ChannelGate(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(1, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelGate module\n",
        "output_tensor = channel_gate(input_tensor)\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelGate module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cksjekign6pB",
        "outputId": "b25eb31a-ca64-45fc-ec6e-36c3ab128b6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChannelGate module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out) # broadcasting\n",
        "        return x * scale\n"
      ],
      "metadata": {
        "id": "abWFYSrzmlJS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class ChannelPool\n",
        "\n",
        "# Create an instance of the ChannelPool class\n",
        "channel_pool = ChannelPool()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(5, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the ChannelPool module\n",
        "output_tensor = channel_pool(input_tensor)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor.shape)\n",
        "\n",
        "# Check if the output tensor has the expected shape\n",
        "expected_shape = (5, 2, 112, 112)\n",
        "assert output_tensor.shape == expected_shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"ChannelPool module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFdbFVHcoHy5",
        "outputId": "43170d9f-1533-44eb-847a-88c819ae9433"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 2, 112, 112])\n",
            "ChannelPool module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'lse'], no_spatial=True):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
        "        self.no_spatial=no_spatial\n",
        "        #if not no_spatial:\n",
        "            #self.SpatialGate = SpatialGate()\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        #if not self.no_spatial:\n",
        "            #x_out = self.SpatialGate(x_out)\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "LBNulbajmoeX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check the output of the class CBAM module\n",
        "\n",
        "# Create an instance of the CBAM class\n",
        "cbam = CBAM(gate_channels=64)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(5, 64, 112, 112)\n",
        "\n",
        "# Pass the input tensor through the CBAM module\n",
        "output_tensor = cbam(input_tensor)\n",
        "\n",
        "\n",
        "# Check if the output tensor has the same shape as the input tensor\n",
        "assert output_tensor.shape == input_tensor.shape, \"Output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CBAM module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UQaaFiVngMa",
        "outputId": "7ccf3d19-da12-437b-f1bf-3cb58f43df7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBAM module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        pre_a =  models.alexnet(pretrained=True)\n",
        "        self.features = pre_a.features[0:8]\n",
        "        for i,p in enumerate(self.features.parameters()):\n",
        "            #if(i<6): #[changed to [2,4,6] depending on how many conv layers to freeze], 132 for mobilenet\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(128) #128 for digit 256 for alpha\n",
        "        self.bn2 = nn.BatchNorm2d(64) # 64 for digit 128 for alpha\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.cbam1 = CBAM(128) #128 for digit 256 for alpha\n",
        "        self.cbam2 = CBAM(64) # 64 for digit 128 for alpha\n",
        "\n",
        "        self.bottleneck2 = nn.Sequential()\n",
        "        self.bottleneck2.add_module('b2_conv1',nn.Conv2d(128, 32, kernel_size=1,stride=1,padding = 'same')) # change with 128 and 32 for digit, 256 and 64 for alpha\n",
        "        #self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn1',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r1',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv2',nn.Conv2d(32, 32, kernel_size=3,stride=1,padding = 'same',dilation=2))\n",
        "        #self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn2',nn.BatchNorm2d(32))\n",
        "        self.bottleneck2.add_module('b2_r2',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_conv3',nn.Conv2d(32, 128, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "        self.bottleneck2.add_module('b2_bn3',nn.BatchNorm2d(128))\n",
        "        self.bottleneck2.add_module('b2_r3',nn.ReLU(True))\n",
        "\n",
        "        self.bottleneck4 = nn.Sequential()\n",
        "        self.bottleneck4.add_module('b4_conv1',nn.Conv2d(64, 16, kernel_size=1,stride=1,padding = 'same')) # change with 64 and 16 for digit, 128 and 32 for alpha\n",
        "        #self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn1',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r1',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv2',nn.Conv2d(16, 16, kernel_size=3,stride=1,padding = 'same', dilation=2))\n",
        "        #self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn2',nn.BatchNorm2d(16))\n",
        "        self.bottleneck4.add_module('b4_r2',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_conv3',nn.Conv2d(16, 64, kernel_size=1,stride=1,padding = 'same'))\n",
        "        #self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        self.bottleneck4.add_module('b4_bn3',nn.BatchNorm2d(64))\n",
        "        self.bottleneck4.add_module('b4_r3',nn.ReLU(True))\n",
        "        #'''\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(256, 100))  #256 for digit 128*4*4 for aplha\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100)) # 500 for alpha\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        #self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10)) #10 for digit and 4 for alpha\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax())\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(256, 100))#256 for digit 128*4*4 for aplha\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "        #self.conv3 =\n",
        "        self.conv4 = nn.Conv2d(384, 128, kernel_size=3,dilation=2) # change back to square filter and dilation 2 for digit\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3,dilation=2)\n",
        "        self.max3 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "    def forward(self, input_data, alpha=1.0):\n",
        "        #input_data = input_data.expand(input_data.data.shape[0], 3, 32, 32)\n",
        "        feature = self.features(input_data)\n",
        "        #feature = self.feature(feature)\n",
        "\n",
        "        feature = self.relu(self.bn1(self.conv4(feature)))\n",
        "        feature = self.bottleneck2(feature)\n",
        "        #feature = feature_o1 + feature_n1\n",
        "        feature = self.cbam1(feature)\n",
        "        feature = self.relu(self.bn2(self.conv5(feature)))\n",
        "        feature = self.bottleneck4(feature)\n",
        "        #feature = feature_o2 + feature_n2\n",
        "        feature = self.cbam2(feature)\n",
        "        feature = self.max3(feature)\n",
        "\n",
        "\n",
        "        feature = feature.view(-1, 256) #256 for digit\n",
        "        reverse_feature = ReverseLayerF.apply(feature, 1.0)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "        #print(class_output.shape)\n",
        "        return class_output, domain_output"
      ],
      "metadata": {
        "id": "fCkAxiEgV1CH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use torchsummary for the cnn model and try to check whether the output is correct or not\n"
      ],
      "metadata": {
        "id": "EwnCmKmTz_ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel()\n",
        "\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DL5vXTPkBCj",
        "outputId": "435688be-0cd9-4f50-b045-22780057408a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
            "              ReLU-2           [-1, 64, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
            "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
            "              ReLU-5          [-1, 192, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9            [-1, 128, 9, 9]         442,496\n",
            "      BatchNorm2d-10            [-1, 128, 9, 9]             256\n",
            "             ReLU-11            [-1, 128, 9, 9]               0\n",
            "           Conv2d-12             [-1, 32, 9, 9]           4,128\n",
            "      BatchNorm2d-13             [-1, 32, 9, 9]              64\n",
            "             ReLU-14             [-1, 32, 9, 9]               0\n",
            "           Conv2d-15             [-1, 32, 9, 9]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 9, 9]              64\n",
            "             ReLU-17             [-1, 32, 9, 9]               0\n",
            "           Conv2d-18            [-1, 128, 9, 9]           4,224\n",
            "      BatchNorm2d-19            [-1, 128, 9, 9]             256\n",
            "             ReLU-20            [-1, 128, 9, 9]               0\n",
            "          Flatten-21                  [-1, 128]               0\n",
            "           Linear-22                    [-1, 8]           1,032\n",
            "             ReLU-23                    [-1, 8]               0\n",
            "           Linear-24                  [-1, 128]           1,152\n",
            "          Flatten-25                  [-1, 128]               0\n",
            "           Linear-26                    [-1, 8]           1,032\n",
            "             ReLU-27                    [-1, 8]               0\n",
            "           Linear-28                  [-1, 128]           1,152\n",
            "      ChannelGate-29            [-1, 128, 9, 9]               0\n",
            "             CBAM-30            [-1, 128, 9, 9]               0\n",
            "           Conv2d-31             [-1, 64, 5, 5]          73,792\n",
            "      BatchNorm2d-32             [-1, 64, 5, 5]             128\n",
            "             ReLU-33             [-1, 64, 5, 5]               0\n",
            "           Conv2d-34             [-1, 16, 5, 5]           1,040\n",
            "      BatchNorm2d-35             [-1, 16, 5, 5]              32\n",
            "             ReLU-36             [-1, 16, 5, 5]               0\n",
            "           Conv2d-37             [-1, 16, 5, 5]           2,320\n",
            "      BatchNorm2d-38             [-1, 16, 5, 5]              32\n",
            "             ReLU-39             [-1, 16, 5, 5]               0\n",
            "           Conv2d-40             [-1, 64, 5, 5]           1,088\n",
            "      BatchNorm2d-41             [-1, 64, 5, 5]             128\n",
            "             ReLU-42             [-1, 64, 5, 5]               0\n",
            "          Flatten-43                   [-1, 64]               0\n",
            "           Linear-44                    [-1, 4]             260\n",
            "             ReLU-45                    [-1, 4]               0\n",
            "           Linear-46                   [-1, 64]             320\n",
            "          Flatten-47                   [-1, 64]               0\n",
            "           Linear-48                    [-1, 4]             260\n",
            "             ReLU-49                    [-1, 4]               0\n",
            "           Linear-50                   [-1, 64]             320\n",
            "      ChannelGate-51             [-1, 64, 5, 5]               0\n",
            "             CBAM-52             [-1, 64, 5, 5]               0\n",
            "        MaxPool2d-53             [-1, 64, 2, 2]               0\n",
            "           Linear-54                  [-1, 100]          25,700\n",
            "             ReLU-55                  [-1, 100]               0\n",
            "           Linear-56                  [-1, 100]          10,100\n",
            "      BatchNorm1d-57                  [-1, 100]             200\n",
            "             ReLU-58                  [-1, 100]               0\n",
            "           Linear-59                   [-1, 10]           1,010\n",
            "       LogSoftmax-60                   [-1, 10]               0\n",
            "           Linear-61                  [-1, 100]          25,700\n",
            "      BatchNorm1d-62                  [-1, 100]             200\n",
            "             ReLU-63                  [-1, 100]               0\n",
            "           Linear-64                    [-1, 2]             202\n",
            "       LogSoftmax-65                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 1,602,560\n",
            "Trainable params: 607,936\n",
            "Non-trainable params: 994,624\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 7.57\n",
            "Params size (MB): 6.11\n",
            "Estimated Total Size (MB): 14.25\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Code to check the output of class CNN model\n",
        "\n",
        "# Create an instance of the CNNModel class\n",
        "cnn_model = CNNModel()\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(10, 3, 224, 224)\n",
        "\n",
        "# Create a random alpha value\n",
        "alpha = torch.randn(1)\n",
        "\n",
        "# Pass the input tensor and alpha value through the CNNModel module\n",
        "class_output, domain_output = cnn_model(input_tensor, alpha)\n",
        "\n",
        "# Print the shapes of the class_output and domain_output tensors\n",
        "print(class_output.shape)\n",
        "print(domain_output.shape)\n",
        "\n",
        "# Check if the class_output tensor has the expected shape\n",
        "expected_class_shape = (10, 10)  # 10 for digit and 4 for alpha\n",
        "assert class_output.shape == expected_class_shape, \"Class output tensor has unexpected shape\"\n",
        "\n",
        "# Check if the domain_output tensor has the expected shape\n",
        "expected_domain_shape = (10, 2)\n",
        "assert domain_output.shape == expected_domain_shape, \"Domain output tensor has unexpected shape\"\n",
        "\n",
        "# Print a message indicating that the code is running perfectly\n",
        "print(\"CNNModel module is running perfectly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2OwNmTXoSg",
        "outputId": "ed28bc0f-9a21-4497-94b7-a32b47bd3392"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10, 2])\n",
            "CNNModel module is running perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bTKfDFyzKOeg"
      }
    }
  ]
}